# -*- coding: utf-8 -*-
"""COLAB. Práctica 1

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1G3n23yPnMZEKamMSKRf7Jl6HNUa2bdrL

# Práctica 1. Procesamiento de datos mediante Apache Spark

**Realizado por:**

- Alberto Ramos González, 100363705.
- Raimundo Andres Prudencio Ramirez, 100464063.
- Enrique de Aramburu Alonso, 100373464.
- Diego Alonso Braojos, 100383427.

## 0. Puesta en marcha
"""

import time
import findspark
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
from datetime import datetime
from pyspark.sql import SparkSession
from pyspark import SparkContext, SparkConf
from pyspark.sql import functions as f

findspark.init()

conf = SparkConf().setMaster('local').setAppName("HPC2")
sc = SparkContext(conf=conf)
spark = SparkSession.builder.master('local[*]').getOrCreate()

"""## 1. Introducción

En esta práctica se realizarán una serie de consultas y estudios utilizando las funciones de Apache Spark. El conjunto de datos sobre el que se realizarán los procesos será el correspondiente a un subconjunto de las carreras de taxis de Nueva York de enero de 2017.

A su vez, se deberán realizar análisis de rednimiento en función de las caraceteríticas de procesamiento que se dediquen a realizar la ejecución del ejercicio.

## 2. Estudios realizados

Se capta el archivo CSV que recoge toda la información de las carreras de taxi que se deben tratar. A partir de él se crea un datframe que será con el que se trabaje en las consultas a desarrollar. También se creará una vista para poder utilizar el datframe con consultas SQL.

### 2.0. Carga CSV
"""

filepath = "tripdata_2017-01.csv"
csv = pd.read_csv(filepath)

df = spark.read.option('header', True).csv(filepath).cache()

df.createOrReplaceTempView('taxis')

df.show()

"""Se capta el archivo CSV que indicará la zona correspondiente a cada ID de localización. Se crearán dos dataframes 
para poder insertar 2 veces las 2 columnas que interesan (Barrio y Zona para el punto de partida y de destino). """

filepath_zones = "taxi+_zone_lookup.csv"

df_zonas1 = spark.read.option('header', True).csv(filepath_zones).cache()
df_zonas2 = spark.read.option('header', True).csv(filepath_zones).cache()

"""###2.1. Limpieza"""

# Syso para verificar que no existe ningun dato nulo, si hubiera solo hacer un filter con is not null --->
# df=df.filter(str(i)+" is not null")
for i in df.columns:
    if df.filter(str(i) + " is null").count() == 0:
        print("El atributo " + str(i) + " no contiene ningun valor a null")
    else:
        print("El atributo " + str(i) + " contiene un valor a null")
    # df=df.filter(str(i)+" is not null")

# Verificar que solo hay vendorID 1 y 2, eliminamos si hay otros
Total = df.count()
df = df.filter("VendorID = 1 or VendorID=2")
limpieza = df.count()
print("Numero de filas eliminadas " + str(Total - limpieza))

# Eliminamos las filas que el resultado de restar tpep_dropoff_datetime - tpep_pickup_datetime<=0 y borramos la
# columana de duration

timeFmt = "yyyy-MM-dd HH:mm:ss"
timeDiff = (f.unix_timestamp('tpep_dropoff_datetime', format=timeFmt) - f.unix_timestamp('tpep_pickup_datetime',
                                                                                         format=timeFmt))
df = df.withColumn("Duration", timeDiff)
df = df.filter(df.Duration > 0)
df = df.drop('Duration')  # Borramos la columna de Duration creada
df.show()

# Eliminos las distancias recorridas por el taximetro que sea <=0.0
Total = df.count()
df = df.filter(df.trip_distance > 0.0)
limpieza = df.count()
print("Numero de filas eliminadas " + str(Total - limpieza))

# Eliminos los RATECODEID que sean distintos de 1,2,3,4,5,6
Total = df.count()
df = df.filter("RatecodeID >= 1 and RatecodeID <=6")
limpieza = df.count()
print("Numero de filas eliminadas " + str(Total - limpieza))

# Eliminos los store_and_fwd_flag que sean distintos de N e Y
Total = df.count()
df = df.filter("store_and_fwd_flag ='N' or store_and_fwd_flag ='Y'")
limpieza = df.count()
print("Numero de filas eliminadas " + str(Total - limpieza))

# Eliminos los payment_type que sean distintos de 1,2,3,4,5,6
Total = df.count()
df = df.filter("payment_type >= 1 and payment_type <=6")
limpieza = df.count()
print("Numero de filas eliminadas " + str(Total - limpieza))

# Eliminos la tarifa que sea <=0.0 ¡¡¡¡¡¡¡¡ REVISAR!!!!!!!
Total = df.count()
df = df.filter(df.fare_amount > 0.0)
limpieza = df.count()
print("Numero de filas eliminadas " + str(Total - limpieza))

# Eliminos extra que sea diferente de 0 o 0.5 o 1
Total = df.count()
df = df.filter('extra=0 or extra=0.5 or extra=1')
limpieza = df.count()
print("Numero de filas eliminadas " + str(Total - limpieza))

# Eliminos improvement_surcharge que sea diferente de 0 o 0.3, verificaria el año pero los datos son del año 2017
Total = df.count()
df = df.filter('improvement_surcharge=0 or improvement_surcharge=0.3')
limpieza = df.count()
print("Numero de filas eliminadas " + str(Total - limpieza))

# Eliminos tip_amount que sea <=0.0
Total = df.count()
df = df.filter(df.tip_amount >= 0.0)
limpieza = df.count()
print("Numero de filas eliminadas " + str(Total - limpieza))

# Eliminos el tolls_amount que sea <=0.0
Total = df.count()
df = df.filter(df.tolls_amount >= 0.0)
limpieza = df.count()
print("Numero de filas eliminadas " + str(Total - limpieza))

# Eliminos el total_amount que sea <=0.0
Total = df.count()
df = df.filter(df.total_amount > 0.0)
limpieza = df.count()
print("Numero de filas eliminadas " + str(Total - limpieza))

"""### 2.2. Número de viajes de cada empresa

Consulta SQL seleccionando la empresa que realiza cada viaje. Se realiza un contador. VeriFone Inc. realiza más viajes que Creative Mobile Technologies, LLC.
"""

empresas = spark.sql('SELECT VendorID FROM taxis WHERE VendorID is not null')
# se deberia quitar el is not null cuando se acabe la limpieza de datos
map1 = empresas.rdd.map(lambda x: (x.VendorID, 1))
reduce1 = map1.reduceByKey(lambda x, y: x + y)
res1 = reduce1.collect()
print(res1)

"""También se podría realizar una consulta SQL directa seleccionando a través del 'group by' las empresas y con el 
contador (count(*)) se obtiene la cantidad de registros. También servirá para contrastar los resultados obtenidos 
anteriormente. """

empresas_sql = spark.sql(
    'SELECT VendorID as empresa , count(*) as cantidad_viajes FROM taxis  WHERE VendorID is not null  group by VendorID')
# Se debe quitar el not null luego de la limpieza
print(empresas_sql.show())

"""**Gráfico de barras**"""

x = ['Creative Mobile Technologies', 'VeriFone Inc.']
y = [res1[0][1], res1[1][1]]

numero_de_grupos = len(x)
indice_barras = np.arange(numero_de_grupos)
ancho_barras = 0.35

plt.figure(figsize=(9, 7))

plt.bar(x, y, width=ancho_barras, color=['tab:orange', 'tab:green'])

plt.xlabel("Empresas", fontdict={'fontsize': 10, 'fontweight': 'bold', 'color': 'tab:blue'})
plt.ylabel("Número de viajes", fontdict={'fontsize': 10, 'fontweight': 'bold', 'color': 'tab:blue'})
plt.title('Viajes efectuados por cada empresa', fontdict={'fontsize': 15, 'fontweight': 'bold', 'color': 'tab:blue'})
plt.show()

"""### 2.3. Viajes en grupo por empresa"""

grupo = df.rdd.filter(lambda x: int(x['passenger_count']) > 2)
map2 = grupo.map(lambda x: (x.VendorID, 1))
reduce2 = map2.reduceByKey(lambda x, y: x + y)
res2 = reduce2.collect()
print(res2)

x = ['VeriFone Inc.', 'Creative Mobile Technologies']
y = [res2[0][1], res2[1][1]]

numero_de_grupos = len(x)
indice_barras = np.arange(numero_de_grupos)
ancho_barras = 0.35

plt.figure(figsize=(9, 7))

plt.bar(x, y, width=ancho_barras, color=['tab:orange', 'tab:green'])

plt.xlabel("Empresas", fontdict={'fontsize': 10, 'fontweight': 'bold', 'color': 'tab:blue'})
plt.ylabel("Número de viajes", fontdict={'fontsize': 10, 'fontweight': 'bold', 'color': 'tab:blue'})
plt.title('Viajes en grupo efectuados por cada empresa',
          fontdict={'fontsize': 15, 'fontweight': 'bold', 'color': 'tab:blue'})
plt.show()

"""### 2.4. Registro de pasajeros más comunes"""

pasajeros = spark.sql('SELECT passenger_count FROM taxis WHERE VendorID is not null')
# se deberia quitar el is not null cuando se acabe la limpieza de datos
map3 = pasajeros.rdd.map(lambda x: (x.passenger_count, 1))
reduce3 = map3.reduceByKey(lambda x, y: x + y).sortBy(lambda x: x[0], True)
res3 = reduce3.collect()
print(res3)

pasajeros = spark.sql('SELECT passenger_count FROM taxis WHERE VendorID=1')
map31 = pasajeros.rdd.map(lambda x: (x.passenger_count, 1))
reduce31 = map31.reduceByKey(lambda x, y: x + y).sortBy(lambda x: x[0], True)
res31 = reduce31.collect()
print(res31)

x = [i[0] for i in res3]
y = [i[1] for i in res3]

print(x)
print(y)

numero_de_grupos = len(x)
indice_barras = np.arange(numero_de_grupos)
ancho_barras = 0.35

plt.figure(figsize=(9, 7))

plt.bar(x, y, width=ancho_barras, color=['tab:orange'])

plt.xlabel("Número de pasajeros", fontdict={'fontsize': 10, 'fontweight': 'bold', 'color': 'tab:blue'})
plt.ylabel("Número de viajes", fontdict={'fontsize': 10, 'fontweight': 'bold', 'color': 'tab:blue'})
plt.title('Viajes según el número de pasajeros', fontdict={'fontsize': 15, 'fontweight': 'bold', 'color': 'tab:blue'})
plt.show()

x = [i[0] for i in res31]
y = [i[1] for i in res31]

print(x)
print(y)

numero_de_grupos = len(x)
indice_barras = np.arange(numero_de_grupos)
ancho_barras = 0.35

plt.figure(figsize=(9, 7))

plt.bar(x, y, width=ancho_barras, color=['tab:orange'])

plt.xlabel("Número de pasajeros", fontdict={'fontsize': 10, 'fontweight': 'bold', 'color': 'tab:blue'})
plt.ylabel("Número de viajes", fontdict={'fontsize': 10, 'fontweight': 'bold', 'color': 'tab:blue'})
plt.title('Viajes de la empresa Creative Mobile Technologies según el número de pasajeros',
          fontdict={'fontsize': 15, 'fontweight': 'bold', 'color': 'tab:blue'})
plt.show()

"""### 2.5. Velocidad media en función de las horas
Primero, usando una consulta SQL, se seleccionan las columnas tpep_pickup_datetime, tpep_dropoff_datetime y trip_distance. Las dos primeras nos facilitarán la variable tiempo y la tercera la variable distancia. Con esas dosvariables se podrá conseguir la velocidad media del viaje.
"""

salidas = spark.sql(
    "SELECT tpep_pickup_datetime, tpep_dropoff_datetime, trip_distance FROM taxis WHERE tpep_pickup_datetime is not null and tpep_dropoff_datetime  is not null and  trip_distance  is not null")
# se deberia quitar el is not null cuando se acabe la limpieza de datos
salidas.show()

"""El método MapReduce se aplicará en los siguientes pasos:

*   **Map.** De las tres columnas mencionadas, pasando de string a datetime.strptime los dos campos fecha.
*   **Map.** Se guardará como clave la hora de comienzo del viaje y como valor la velocidad media del mismo, aplicando la fórmula: (datetime fin - datetime inicio) / distancia.
*   **Reduce.** Para cada clave se calculará la media de sus valores. Por último, se ordenará en orden creciente de horas (0-23).
"""

map4 = salidas.rdd.map(lambda x: (datetime.strptime(x.tpep_pickup_datetime, '%Y-%m-%d %H:%M:%S'),
                                  datetime.strptime(x.tpep_dropoff_datetime, '%Y-%m-%d %H:%M:%S'), x.trip_distance))
map41 = map4.map(lambda x: (x[0].hour, ((float(x[2]) / float((x[1] - x[0]).seconds + 1)) * 5793.64, 1)))
reduce4 = map41.reduceByKey(lambda x, y: ((x[0] + y[0]), x[1] + y[1])).sortBy(lambda x: x[0], True).collect()
res4 = [(x[0], x[1][0] / x[1][1]) for x in reduce4]
print(res4)

"""**Gráfico de líneas**"""

x = [i[0] for i in res4]
y = [i[1] for i in res4]

plt.figure(figsize=(8, 6))
plt.plot(x, y, marker='o')
plt.plot()

plt.xticks(range(0, 24, 1))

plt.xlabel("Horas (h)", fontdict={'fontsize': 10, 'fontweight': 'bold', 'color': 'tab:blue'})
plt.ylabel("Velocidad media (km/h)", fontdict={'fontsize': 10, 'fontweight': 'bold', 'color': 'tab:blue'})
plt.title('Velocidad media por horas', fontdict={'fontsize': 15, 'fontweight': 'bold', 'color': 'tab:blue'})
plt.show()

"""También podemos usar la hora de llegada para agrupar por horas"""

map4 = salidas.rdd.map(lambda x: (datetime.strptime(x.tpep_pickup_datetime, '%Y-%m-%d %H:%M:%S'),
                                  datetime.strptime(x.tpep_dropoff_datetime, '%Y-%m-%d %H:%M:%S'), x.trip_distance))
map41 = map4.map(lambda x: (x[1].hour, ((float(x[2]) / float((x[1] - x[0]).seconds + 1)) * 5793.64, 1)))
reduce4 = map41.reduceByKey(lambda x, y: ((x[0] + y[0]), x[1] + y[1])).sortBy(lambda x: x[0], True).collect()
resLlegada = [(x[0], x[1][0] / x[1][1]) for x in reduce4]
print(resLlegada)

x = [i[0] for i in resLlegada]
y = [i[1] for i in resLlegada]

plt.figure(figsize=(8, 6))
plt.plot(x, y, marker='o')
plt.plot()

plt.xticks(range(0, 24, 1))

plt.xlabel("Horas (h)", fontdict={'fontsize': 10, 'fontweight': 'bold', 'color': 'tab:blue'})
plt.ylabel("Velocidad media (km/h)", fontdict={'fontsize': 10, 'fontweight': 'bold', 'color': 'tab:blue'})
plt.title('Velocidad media por horas', fontdict={'fontsize': 15, 'fontweight': 'bold', 'color': 'tab:blue'})
plt.show()

"""2da forma Velocidad media en funcion de las horas : utilizando sentencia sql """

# importando librerias para sql y las horas
from pyspark.sql.functions import *
from pyspark.sql.functions import hour

# Se convierte los datos de las fecha (formato String) a formato timeStamp , luego se añade  una columna velocidad
# media (Los timestamp convertidos se castean a segundos y se realiza el calculo distancia/tiempo )
df2 = df.withColumn('tpep_pickup_datetime_timestamp', to_timestamp(col('tpep_pickup_datetime'))) \
    .withColumn('tpep_dropoff_datetime_timestamp', to_timestamp(col('tpep_dropoff_datetime'))) \
    .withColumn('Velocidad_media', col('trip_distance') / (
        col("tpep_dropoff_datetime_timestamp").cast("long") - col('tpep_pickup_datetime_timestamp').cast("long")))
df2 = df2.withColumn("hour", hour(col("tpep_pickup_datetime_timestamp")))

# se selecionan las columnas necesarias para el analisis
df2.select("tpep_pickup_datetime_timestamp", "tpep_dropoff_datetime_timestamp", "hour", "Velocidad_media").show(
    truncate=False)

"""Agrupamos y calculamos el promedio con la funcion avg"""

# avg : funcion para calcular la media
df2.groupBy('hour').avg('Velocidad_media').show()

"""### 2.6. Viajes en taxi más comunes

Se utilizarán dos dataframes para poder insertar 2 veces las 2 columnas que interesan (Barrio y Zona para el punto de partida y de destino). Se realizará haciendo un 'rename' de las columnas y después un 'join' utilizando la columna original LocationID, para juntarlo al dataframe principal de carreras de taxis.
"""

df_zonas1 = df_zonas1.withColumnRenamed('Borough', 'PU_Borough').withColumnRenamed('Zone', 'PU_Zone')
df_zonas2 = df_zonas2.withColumnRenamed('Borough', 'DO_Borough').withColumnRenamed('Zone', 'DO_Zone')

df_completo_prev = df.join(df_zonas1, df['PULocationID'] == df_zonas1['LocationID'])
df_completo = df_completo_prev.join(df_zonas2, df_completo_prev['DOLocationID'] == df_zonas2['LocationID'])
df_completo = df_completo.drop('LocationID')
df_completo.show()

"""Ahora se guardarán en el dataframe las columnas de barrio y zona de partida y de destino para cada una de las 
carreras de taxis utilizando una sentencia SQL. """

df_completo.createOrReplaceTempView('zonas')
zonas = spark.sql("SELECT PU_Borough, PU_Zone, DO_Borough, DO_Zone FROM zonas")
zonas.show()

"""El método MapReduce se aplicará en los siguientes pasos:

*   **Map.** Se definirá la clave como un string formado por la concatenación del nombre de barrio de partida, zona de partida, barrio de destino y zona de destino, separando partida y destino por un guión (-). Cada clave tendrá especificado el valor 1.
*   **Reduce.** Para cada clave se sumarán sus valores, es decir, se realizará un contador. Por último, se ordenará en orden creciente de apariciones.
"""

map5 = zonas.rdd.map(lambda x: (x.PU_Borough + " " + x.PU_Zone + " - " + x.DO_Borough + " " + x.DO_Zone, 1))
reduce5 = map5.reduceByKey(lambda x, y: x + y).sortBy(lambda x: x[1], False)
res5 = reduce5.collect()
print(res5)

"""**Gráfico de sectores (10 mejores resultados)**"""

res5_graf = reduce5.take(10)

x = [i[0] for i in res5_graf]
y = [i[1] for i in res5_graf]

plt.figure(figsize=(9, 7))
plt.title('10 viajes más típicos (Porcentaje de frecuencia sobre el total)',
          fontdict={'fontsize': 15, 'fontweight': 'bold', 'color': 'tab:blue'})

plt.pie(y, labels=x, autopct="%0.1f %%", textprops={'fontsize': 12})
plt.axis("equal")

plt.show()

"""### 2.7. Mejor propina media por trayecto (también se puede haccer por zona de salida)"""

df_zonas1 = df_zonas1.withColumnRenamed('Borough', 'PU_Borough').withColumnRenamed('Zone', 'PU_Zone')
df_zonas2 = df_zonas2.withColumnRenamed('Borough', 'DO_Borough').withColumnRenamed('Zone', 'DO_Zone')

df_completo_prev = df.join(df_zonas1, df['PULocationID'] == df_zonas1['LocationID'])
df_completo = df_completo_prev.join(df_zonas2, df_completo_prev['DOLocationID'] == df_zonas2['LocationID'])
df_completo = df_completo.drop('LocationID')
df_completo.show()

df_completo.createOrReplaceTempView('zonas')
zonas = spark.sql("SELECT PU_Borough, PU_Zone, DO_Borough, DO_Zone, tip_amount FROM zonas")
zonas.show()

"""Propina media usando salida y destino"""

map6 = zonas.rdd.map(
    lambda x: (x.PU_Borough + " " + x.PU_Zone + " - " + x.DO_Borough + " " + x.DO_Zone, (float(x.tip_amount), 1)))
reduce6 = map6.reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1])).sortBy(lambda x: x[1], False).collect()
res6 = [(x[0], x[1][0] / x[1][1]) for x in reduce6]
print(res6)

"""Propina media usando solo salida"""

map7 = zonas.rdd.map(lambda x: (x.PU_Borough + " " + x.PU_Zone, (float(x.tip_amount), 1)))
reduce7 = map7.reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1])).sortBy(lambda x: x[1], False).collect()
res7 = [(x[0], x[1][0] / x[1][1]) for x in reduce7]
print(res7)

"""Propina media usando solo distrito de Salida"""

map8 = zonas.rdd.map(lambda x: (x.PU_Borough, (float(x.tip_amount), 1)))
reduce8 = map8.reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1])).sortBy(lambda x: x[0], True).collect()
res8 = [(x[0], x[1][0] / x[1][1]) for x in reduce8]
print(res8)

"""Propina media usando llegada"""

map9 = zonas.rdd.map(lambda x: (x.DO_Borough + " " + x.DO_Zone, (float(x.tip_amount), 1)))
reduce9 = map9.reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1])).sortBy(lambda x: x[0], False).collect()
res9 = [(x[0], x[1][0] / x[1][1]) for x in reduce9]
print(res9)

"""Propina media usando solo distrito de llegada"""

map10 = zonas.rdd.map(lambda x: (x.DO_Borough, (float(x.tip_amount), 1)))
reduce10 = map10.reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1])).sortBy(lambda x: x[0], True).collect()
res10 = [(x[0], x[1][0] / x[1][1]) for x in reduce10]
print(res10)

"""**Gráfica de líneas**"""

x = [i[0] for i in res8]
y1 = [i[1] for i in res8]
y2 = [i[1] for i in res10]
plt.plot(x, y1, label="Salida")
plt.plot(x, y2, label="Llegadas")
plt.plot()

plt.xlabel("Distritos")
plt.ylabel("Propina media")
plt.title("Propina media segun distrito")
plt.legend()
plt.show()

"""**Gráfica de barras**"""

X = [i[0] for i in res8]
y1 = [i[1] for i in res8]
y2 = [i[1] for i in res10]

numero_de_grupos = len(X)
indice_barras = np.arange(numero_de_grupos)
ancho_barras = 0.35

plt.figure(figsize=(9, 7))

plt.bar(indice_barras, y1, width=ancho_barras, label='Salida')
plt.bar(indice_barras + ancho_barras, y2, width=ancho_barras, label='Llegada')
plt.legend(loc='best')

# Se colocan los indicadores en el eje x
plt.xticks(indice_barras + ancho_barras,
           ('Bronx', 'Brooklyn', 'EWR', 'Manhattan', 'Queens', 'Staten Island', 'Unknown'))

plt.xlabel("Distrito", fontdict={'fontsize': 10, 'fontweight': 'bold', 'color': 'tab:blue'})
plt.ylabel("Dólares de propina", fontdict={'fontsize': 10, 'fontweight': 'bold', 'color': 'tab:blue'})
plt.title('Propina media en función del distrito de llegada-salida',
          fontdict={'fontsize': 15, 'fontweight': 'bold', 'color': 'tab:blue'})

plt.show()

"""**Diagrama de dispersión**"""

x1 = res8[0][1]
y1 = res10[0][1]
x2 = res8[1][1]
y2 = res10[1][1]
x3 = res8[2][1]
y3 = res10[2][1]
x4 = res8[3][1]
y4 = res10[3][1]
x5 = res8[4][1]
y5 = res10[4][1]
x6 = res8[5][1]
y6 = res10[5][1]
x7 = res8[6][1]
y7 = res10[6][1]

plt.figure(figsize=(9, 7))

plt.scatter(x1, y1, label=res8[0][0], marker='o', s=100)
plt.scatter(x2, y2, label=res8[1][0], marker='o', s=100)
plt.scatter(x3, y3, label=res8[2][0], marker='o', s=100)
plt.scatter(x4, y4, label=res8[3][0], marker='o', s=100)
plt.scatter(x5, y5, label=res8[4][0], marker='o', s=100)
plt.scatter(x6, y6, label=res8[5][0], marker='o', s=100)
plt.scatter(x7, y7, label=res8[6][0], marker='o', s=100)
plt.legend()
plt.xlabel("Propina media en llegada", fontdict={'fontsize': 10, 'fontweight': 'bold', 'color': 'tab:blue'})
plt.ylabel("Propina media en salida", fontdict={'fontsize': 10, 'fontweight': 'bold', 'color': 'tab:blue'})
plt.title('Propina media en función del distrito de llegada-salida',
          fontdict={'fontsize': 15, 'fontweight': 'bold', 'color': 'tab:blue'})
plt.show()

"""### 2.8. Mejor precio (menos dólares por kilómetro recorrido)"""

map11 = df.rdd.map(lambda x: (int(x['VendorID']), float(x['trip_distance']), float(x['total_amount'])))
map12 = map11.map(lambda x: (x[0], ((x[2] / (x[1] + 1)) / (1.1639 * 1.6093), 1)))
reduce11 = map12.reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1])).collect()
res11 = [(x[0], x[1][0] / x[1][1]) for x in reduce11]
print(res11)

x = ['VeriFone Inc.', 'Creative Mobile Technologies']
y = [res11[0][1], res11[1][1]]

numero_de_grupos = len(x)
indice_barras = np.arange(numero_de_grupos)
ancho_barras = 0.35

plt.figure(figsize=(9, 7))

plt.bar(x, y, width=ancho_barras, color=['tab:orange', 'tab:green'])

plt.xlabel("Empresas", fontdict={'fontsize': 10, 'fontweight': 'bold', 'color': 'tab:blue'})
plt.ylabel("Euros por kilómetro medio (€/km)", fontdict={'fontsize': 10, 'fontweight': 'bold', 'color': 'tab:blue'})
plt.title('Media de euros por kilómetro recorrido',
          fontdict={'fontsize': 15, 'fontweight': 'bold', 'color': 'tab:blue'})
plt.show()

"""###2.9. Tiempo de trayecto medio por horas"""

map12 = df.rdd.map(lambda x: (datetime.strptime(x['tpep_pickup_datetime'], '%Y-%m-%d %H:%M:%S'),
                              datetime.strptime(x['tpep_dropoff_datetime'], '%Y-%m-%d %H:%M:%S'), x['trip_distance']))
map12 = map12.map(lambda x: (x[0].hour, (float((x[1] - x[0]).seconds / 60), 1)))
reduce12 = map12.reduceByKey(lambda x, y: ((x[0] + y[0]), x[1] + y[1])).sortBy(lambda x: x[0], True).collect()
res12 = [(x[0], x[1][0] / x[1][1]) for x in reduce12]
print(res12)

x = [i[0] for i in res12]
y = [i[1] for i in res12]

plt.figure(figsize=(8, 6))
plt.plot(x, y, marker='o')
plt.plot()

plt.xticks(range(0, 24, 1))
plt.yticks(range(10, 20, 2))

plt.xlabel("Horas (h)", fontdict={'fontsize': 10, 'fontweight': 'bold', 'color': 'tab:blue'})
plt.ylabel("Tiempo medio (minutos)", fontdict={'fontsize': 10, 'fontweight': 'bold', 'color': 'tab:blue'})
plt.title('Tiempo de trayecto medio por horas', fontdict={'fontsize': 15, 'fontweight': 'bold', 'color': 'tab:blue'})
plt.show()

"""### 2.10 Tiempo medio de la duracion de los viajes clasificados por VendorID"""

from pyspark.sql import functions as F

timeFmt = "yyyy-MM-dd HH:mm:ss"
timeDiff = (F.unix_timestamp('tpep_dropoff_datetime', format=timeFmt) - F.unix_timestamp('tpep_pickup_datetime',
                                                                                         format=timeFmt))
df = df.withColumn("Duration", timeDiff / 60)
# df=df.drop('Duration')
map13 = df.rdd.map(lambda x: (int(x['VendorID']), float(x['Duration'])))
reduce13 = map13.reduceByKey(lambda x, y: x + y).sortBy(lambda x: x[1], False).collect()
res13 = [(reduce13[0][0], reduce13[0][1] / df.filter('VendorID=' + str(reduce13[0][0])).count()),
         (reduce13[1][0], reduce13[1][1] / df.filter('VendorID=' + str(reduce13[1][0])).count())]
print("Minutos en total clasificado por VendorID " + str(reduce13))
print("Media de tiempo en minutos clasificado por VendorID" + str(res13))

x = ['Creative Mobile Technologies', 'VeriFone Inc.']
y = [res13[1][1], res13[0][1]]

numero_de_grupos = len(x)
indice_barras = np.arange(numero_de_grupos)
ancho_barras = 0.35

plt.figure(figsize=(9, 7))

plt.bar(x, y, width=ancho_barras, color=['tab:orange', 'tab:green'])

plt.xlabel("Empresas", fontdict={'fontsize': 10, 'fontweight': 'bold', 'color': 'tab:blue'})
plt.ylabel("Tiempo en minutos", fontdict={'fontsize': 10, 'fontweight': 'bold', 'color': 'tab:blue'})
plt.title('Tiempo medio de los trayectos efectuados por cada empresa',
          fontdict={'fontsize': 15, 'fontweight': 'bold', 'color': 'tab:blue'})
plt.show()

"""## 3. Conclusiones"""
